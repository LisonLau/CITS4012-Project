{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 CITS4012 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "notes for marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size\t (7090, 3)\n",
      "Test data size\t\t (901, 3)\n",
      "Validation data size\t (888, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "with open('train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    train_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "# Load the test data\n",
    "with open('test.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    test_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "    \n",
    "# Load the validation data\n",
    "with open('val.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    val_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "# Get the x and y lists for training, test and validation data\n",
    "training_x = train_data['sentence'].tolist()\n",
    "training_y = [(train_data['aspect'][i], train_data['polarity'][i]) for i in range(len(train_data))]\n",
    "test_x = test_data['sentence'].tolist()\n",
    "test_y = [(test_data['aspect'][i], test_data['polarity'][i]) for i in range(len(test_data))]\n",
    "val_x = val_data['sentence'].tolist()\n",
    "val_y = [(val_data['aspect'][i], val_data['polarity'][i]) for i in range(len(val_data))]\n",
    "\n",
    "# Set number of polarities and aspects\n",
    "num_polarities = 3\n",
    "num_aspects = 8\n",
    "\n",
    "print(\"Training data size\\t\", train_data.shape)\n",
    "print(\"Test data size\\t\\t\", test_data.shape)\n",
    "print(\"Validation data size\\t\", val_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\allis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Punctuation Removal\n",
    "# maybe keep emoticons !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# handle contractions (i've -> i have)\n",
    "def remove_punctuation_re(x):\n",
    "    x = re.sub(r'[^\\w\\s]','',x)\n",
    "    return x\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Stopwords Removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = sw.words('english')\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Lemmatisation\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# English Contractions Dictionary\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
    "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
    "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
    "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
    "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the data\n",
    "def preprocess_data(sentence_list):\n",
    "    output_list = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence = sentence.lower()                     # Case folding\n",
    "        for word, new_word in contraction_dict.items(): # Deal with contractions\n",
    "            sentence = sentence.replace(word, new_word)\n",
    "        sentence = remove_punctuation_re(sentence)      # Remove punctuation\n",
    "        tokens = word_tokenize(sentence)                # Tokenise\n",
    "        output_list.append(tokens)\n",
    "    return output_list\n",
    "\n",
    "# Preprocess the data and get the tokenised sentence lists\n",
    "train_x_token = preprocess_data(training_x)\n",
    "test_x_token = preprocess_data(test_x)\n",
    "val_x_token = preprocess_data(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vocabulary to index dictionary {word: index}\n",
    "word_to_idx = {}\n",
    "for sentence in train_x_token:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "word_list = list(word_to_idx.keys())\n",
    "\n",
    "# Aspect vocabulary to index dictionary {aspect: index}\n",
    "aspect_to_idx = {\n",
    "    \"food\": 0,\n",
    "    \"service\": 1,\n",
    "    \"staff\": 2,\n",
    "    \"price\": 3,\n",
    "    \"ambience\": 4,\n",
    "    \"menu\": 5,\n",
    "    \"place\": 6,\n",
    "    \"miscellaneous\": 7\n",
    "}\n",
    "\n",
    "# Polarity vocabulary to index dictionary {polarity: index}\n",
    "polarity_to_idx = {\n",
    "    'positive': 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token index lists for training data\n",
    "train_x_idx = []\n",
    "for sentence in train_x_token:\n",
    "    sentence_idx = [word_to_idx[word] for word in sentence]\n",
    "    train_x_idx.append(sentence_idx)\n",
    "\n",
    "train_y_idx = []\n",
    "for aspect, polarity in training_y:\n",
    "    aspect_idx = aspect_to_idx[aspect]\n",
    "    polarity_idx = polarity_to_idx[polarity]\n",
    "    train_y_idx.append((aspect_idx, polarity_idx))\n",
    "    \n",
    "# One-hot encoding for training data\n",
    "train_x_onehot = []\n",
    "for sentence in train_x_idx:\n",
    "    sentence_onehot = np.zeros(len(word_to_idx))\n",
    "    for idx in sentence:\n",
    "        sentence_onehot[idx] = 1\n",
    "    train_x_onehot.append(sentence_onehot)\n",
    "\n",
    "train_y_onehot = []\n",
    "for aspect, polarity in train_y_idx:\n",
    "    aspect_onehot = np.zeros(num_aspects)\n",
    "    aspect_onehot[aspect] = 1\n",
    "    polarity_onehot = np.zeros(num_polarities)\n",
    "    polarity_onehot[polarity] = 1\n",
    "    train_y_onehot.append((aspect_onehot, polarity_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    \n",
    "    sentence = train_data.loc[i, 'sentence']\n",
    "    \n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # print(\"1\", sentence)\n",
    "    \n",
    "    # Tokenise\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # print(\"2\", tokens)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    re_tokens = [remove_punctuation_re(word) for word in tokens]\n",
    "    # print(\"3\", re_tokens)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    sw_tokens = [word for word in re_tokens if word.lower() not in stopwords and word != '']\n",
    "    # print(\"4\", sw_tokens)\n",
    "    \n",
    "    # Stemming\n",
    "    stem_tokens = [stemmer.stem(word) for word in sw_tokens]\n",
    "    # print(\"5\", stem_tokens)\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemma_tokens = [lemmatizer.lemmatize(word) for word in stem_tokens]\n",
    "    # print(\"6\", lemma_tokens)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tokens = pos_tag(lemma_tokens)\n",
    "    # print(\"7\", pos_tokens)\n",
    "    \n",
    "    # Reconstruct sentence\n",
    "    sentence = \" \".join(lemma_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m model1 \u001b[38;5;241m=\u001b[39m Model1(\u001b[38;5;28mlen\u001b[39m(word_to_idx), \u001b[38;5;241m100\u001b[39m, num_aspects)\n\u001b[0;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     24\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\torch\\optim\\adam.py:45\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[1;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable, differentiable, fused)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweight_decay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m     42\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[0;32m     43\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[0;32m     44\u001b[0m                 differentiable\u001b[38;5;241m=\u001b[39mdifferentiable, fused\u001b[38;5;241m=\u001b[39mfused)\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m differentiable:\n",
      "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:273\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m    271\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    275\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO\n",
    "        out = x\n",
    "        return out\n",
    "    \n",
    "# Set hyperparameters\n",
    "num_epochs = 0\n",
    "display_interval = 0\n",
    "learning_rate = 0\n",
    "# TODO\n",
    "\n",
    "# Train the model\n",
    "model1 = Model1(len(word_to_idx), 100, num_aspects)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model1(train_x_onehot)\n",
    "    loss = criterion(outputs, train_y_idx)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % display_interval == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "        \n",
    "# Save the model\n",
    "torch.save(model1, 'model1.pt')\n",
    "\n",
    "# Load the model\n",
    "model1 = torch.load('model1.pt')\n",
    "\n",
    "# Test the model\n",
    "model1.eval()\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 8 at dim 2 (got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     48\u001b[0m     x_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(train_x_onehot, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 49\u001b[0m     y_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_y_onehot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     x_length \u001b[38;5;241m=\u001b[39m x_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     51\u001b[0m     y_length \u001b[38;5;241m=\u001b[39m y_tensor\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 8 at dim 2 (got 3)"
     ]
    }
   ],
   "source": [
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.gru(input, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, output_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size)\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "hidden_size = 50\n",
    "# embedding = nn.Embedding(len(word_to_idx), hidden_size)\n",
    "\n",
    "x_size = len(word_to_idx)\n",
    "y_size = num_polarities\n",
    "encoder = Encoder(x_size, hidden_size)\n",
    "decoder = Decoder(hidden_size, y_size)\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    x_tensor = torch.tensor(train_x_onehot, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(train_y_onehot, dtype=torch.float32)\n",
    "    x_length = x_tensor.size(0)\n",
    "    y_length = y_tensor.size(0)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    \n",
    "    loss = 0\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # Feed the input data to the encoder\n",
    "    encoder_hiddens = torch.zeros(x_length, encoder.hidden_size)\n",
    "    for i in range(len(train_data)):\n",
    "        encoder_output, encoder_hidden = encoder(x_tensor[i], encoder_hidden)\n",
    "        encoder_hiddens[i] = encoder_output\n",
    "\n",
    "    decoder_input = torch.tensor([[0.0]])  # Assuming the start token is represented by 0.0\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    # Feed the target data to the decoder with teacher forcing\n",
    "    for i in range(len(train_data)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        loss += criterion(decoder_output, y_tensor[i])\n",
    "        decoder_input = y_tensor[i]\n",
    "    \n",
    "    # for i in range(len(train_data)):\n",
    "    #     encoder_output, encoder_hidden = encoder(x_tensor[i], encoder_hidden)\n",
    "    #     decoder_input = torch.tensor([[0.0]])  # Assuming the start token is represented by 0.0\n",
    "    #     decoder_hidden = encoder_hidden\n",
    "        \n",
    "    #     for i in range(len(train_data)):\n",
    "    #         decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    #         loss += criterion(decoder_output, y_tensor[i])\n",
    "    #         decoder_input = y_tensor[i]\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    loss = loss.item() / len(train_data)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss}')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cits5508-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
