{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 CITS4012 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "\n",
    "notes for marker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size\t (7090, 3)\n",
      "Test data size\t\t (901, 3)\n",
      "Validation data size\t (888, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the training data\n",
    "with open('train.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    train_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "# Load the test data\n",
    "with open('test.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    test_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "    \n",
    "# Load the validation data\n",
    "with open('val.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "    val_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
    "\n",
    "print(\"Training data size\\t\", train_data.shape)\n",
    "print(\"Test data size\\t\\t\", test_data.shape)\n",
    "print(\"Validation data size\\t\", val_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It might be the best sit down food I've had in the area, so if you are going to the upright citizen brigade, or the garden, it could be just the place for you.\n",
      "food = positive\n",
      "place = neutral\n",
      "Hostess was extremely accommodating when we arrived an hour early for our reservation.\n",
      "staff = positive\n",
      "miscellaneous = neutral\n",
      "We were a couple of minutes late for our reservation and minus one guest, but we didn't think we deserved the attitude we got from the hostess.\n",
      "miscellaneous = neutral\n",
      "staff = negative\n"
     ]
    }
   ],
   "source": [
    "# Print to visualise data\n",
    "for i in range(0, 6, 2):\n",
    "    print(train_data['sentence'][i])\n",
    "    print(train_data['aspect'][i], \"=\", train_data['polarity'][i])\n",
    "    print(train_data['aspect'][i+1], \"=\", train_data['polarity'][i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation Removal\n",
    "# maybe keep emoticons !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# handle contractions (i've -> i have)\n",
    "def remove_punctuation_re(x):\n",
    "    x = re.sub(r'[^\\w\\s]','',x)\n",
    "    return x\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Stopwords Removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = sw.words('english')\n",
    "\n",
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Lemmatisation\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# POS Tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_data)):\n",
    "    \n",
    "    sentence = train_data.loc[i, 'sentence']\n",
    "    \n",
    "    # Lowercase\n",
    "    sentence = sentence.lower()\n",
    "    # print(\"1\", sentence)\n",
    "    \n",
    "    # Tokenise\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # print(\"2\", tokens)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    re_tokens = [remove_punctuation_re(word) for word in tokens]\n",
    "    # print(\"3\", re_tokens)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    sw_tokens = [word for word in re_tokens if word.lower() not in stopwords and word != '']\n",
    "    # print(\"4\", sw_tokens)\n",
    "    \n",
    "    # Stemming\n",
    "    stem_tokens = [stemmer.stem(word) for word in sw_tokens]\n",
    "    # print(\"5\", stem_tokens)\n",
    "    \n",
    "    # Lemmatisation\n",
    "    lemma_tokens = [lemmatizer.lemmatize(word) for word in stem_tokens]\n",
    "    # print(\"6\", lemma_tokens)\n",
    "    \n",
    "    # POS Tagging\n",
    "    pos_tokens = pos_tag(lemma_tokens)\n",
    "    # print(\"7\", pos_tokens)\n",
    "    \n",
    "    # Reconstruct sentence\n",
    "    sentence = \" \".join(lemma_tokens)\n",
    "    \n",
    "\n",
    "# for i in range(len(test_data)):\n",
    "#     test_data.loc[i, 'sentence'] = remove_punctuation_re(test_data.loc[i, 'sentence'])\n",
    "\n",
    "# for i in range(len(val_data)):\n",
    "#     val_data.loc[i, 'sentence'] = remove_punctuation_re(val_data.loc[i, 'sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Testing and Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cits5508-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
