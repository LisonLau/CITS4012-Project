{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDcYnRfaMOsP"
      },
      "source": [
        "# 2024 CITS4012 Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzE-7blTMOsS"
      },
      "source": [
        "# Readme\n",
        "\n",
        "notes for marker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JArpTqpMOsT"
      },
      "source": [
        "# 1. Dataset Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15myqX_yMOsT"
      },
      "source": [
        "## Import the libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kyPoIBWWMOsU"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjBb3WWxMOsV"
      },
      "source": [
        "## Loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0JEMS1mAMWg_",
        "outputId": "0090c15d-27da-4152-c128-c780f8181e0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "oXxsKZwoMOsW",
        "outputId": "276a3429-7b4b-4b87-95db-19ce4ce8789f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size\t (7090, 3)\n",
            "Test data size\t\t (901, 3)\n",
            "Validation data size\t (888, 3)\n"
          ]
        }
      ],
      "source": [
        "# Load the training data\n",
        "with open('/content/drive/MyDrive/data/train.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    train_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
        "\n",
        "# Load the test data\n",
        "with open('/content/drive/MyDrive/data/test.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    test_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
        "\n",
        "# Load the validation data\n",
        "with open('/content/drive/MyDrive/data/val.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    val_data = pd.DataFrame(data['data'], columns=data['columns'])\n",
        "\n",
        "# Get the x and y lists for training, test and validation data\n",
        "training_x = train_data['sentence'].tolist()\n",
        "training_y = [(train_data['aspect'][i], train_data['polarity'][i]) for i in range(len(train_data))]\n",
        "test_x = test_data['sentence'].tolist()\n",
        "test_y = [(test_data['aspect'][i], test_data['polarity'][i]) for i in range(len(test_data))]\n",
        "val_x = val_data['sentence'].tolist()\n",
        "val_y = [(val_data['aspect'][i], val_data['polarity'][i]) for i in range(len(val_data))]\n",
        "\n",
        "# Set number of polarities and aspects\n",
        "num_polarities = 3\n",
        "num_aspects = 8\n",
        "\n",
        "print(\"Training data size\\t\", train_data.shape)\n",
        "print(\"Test data size\\t\\t\", test_data.shape)\n",
        "print(\"Validation data size\\t\", val_data.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni3Js2GkMOsY"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sNpZV4StMOsY",
        "outputId": "4e021417-e91d-44b6-ee43-6c3160882a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# Punctuation Removal\n",
        "# maybe keep emoticons !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
        "# handle contractions (i've -> i have)\n",
        "def remove_punctuation_re(x):\n",
        "    x = re.sub(r'[^\\w\\s]','',x)\n",
        "    return x\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Stopwords Removal\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "from nltk.tokenize import word_tokenize\n",
        "stopwords = sw.words('english')\n",
        "\n",
        "# Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Lemmatisation\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# POS Tagging\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "# English Contractions Dictionary\n",
        "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n",
        "                    \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n",
        "                    \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\",\n",
        "                    \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\",\n",
        "                    \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
        "                    \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                    \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\",\n",
        "                    \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
        "                    \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
        "                    \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
        "                    \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\",\n",
        "                    \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
        "                    \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                    \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                    \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\",\n",
        "                    \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n",
        "                    \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\",\n",
        "                    \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n",
        "                    \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\",\n",
        "                    \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n",
        "                    \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                    \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RtUZZzHbMOsa"
      },
      "outputs": [],
      "source": [
        "# Function to preprocess the data\n",
        "def preprocess_data(sentence_list):\n",
        "    output_list = []\n",
        "    for sentence in sentence_list:\n",
        "        sentence = sentence.lower()                     # Case folding\n",
        "        for word, new_word in contraction_dict.items(): # Deal with contractions\n",
        "            sentence = sentence.replace(word, new_word)\n",
        "        sentence = remove_punctuation_re(sentence)      # Remove punctuation\n",
        "        tokens = word_tokenize(sentence)                # Tokenise\n",
        "        output_list.append(tokens)\n",
        "    return output_list\n",
        "\n",
        "# Preprocess the data and get the tokenised sentence lists\n",
        "train_x_token = preprocess_data(training_x)\n",
        "test_x_token = preprocess_data(test_x)\n",
        "val_x_token = preprocess_data(val_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHTFH4HHMOsa"
      },
      "outputs": [],
      "source": [
        "# Word vocabulary to index dictionary {word: index}\n",
        "word_to_idx = {}\n",
        "for sentence in train_x_token:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)\n",
        "word_list = list(word_to_idx.keys())\n",
        "\n",
        "# Aspect vocabulary to index dictionary {aspect: index}\n",
        "aspect_to_idx = {\n",
        "    \"food\": 0,\n",
        "    \"service\": 1,\n",
        "    \"staff\": 2,\n",
        "    \"price\": 3,\n",
        "    \"ambience\": 4,\n",
        "    \"menu\": 5,\n",
        "    \"place\": 6,\n",
        "    \"miscellaneous\": 7\n",
        "}\n",
        "\n",
        "# Polarity vocabulary to index dictionary {polarity: index}\n",
        "polarity_to_idx = {\n",
        "    'positive': 0,\n",
        "    'neutral': 1,\n",
        "    'negative': 2\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExTMDoTmMOsb",
        "outputId": "f8cd6fb0-f18c-4b9a-d322-9a3e0be113c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(array([1., 0., 0., 0., 0., 0., 0., 0.]), array([1., 0., 0.]))\n",
            "(array([0., 0., 0., 0., 0., 0., 1., 0.]), array([0., 1., 0.]))\n",
            "(array([0., 0., 1., 0., 0., 0., 0., 0.]), array([1., 0., 0.]))\n",
            "(array([0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 1., 0.]))\n",
            "(array([0., 0., 0., 0., 0., 0., 0., 1.]), array([0., 1., 0.]))\n"
          ]
        }
      ],
      "source": [
        "# Token index lists for training data\n",
        "train_x_idx = []\n",
        "for sentence in train_x_token:\n",
        "    sentence_idx = [word_to_idx[word] for word in sentence]\n",
        "    train_x_idx.append(sentence_idx)\n",
        "\n",
        "train_y_idx = []\n",
        "for aspect, polarity in training_y:\n",
        "    aspect_idx = aspect_to_idx[aspect]\n",
        "    polarity_idx = polarity_to_idx[polarity]\n",
        "    train_y_idx.append((aspect_idx, polarity_idx))\n",
        "\n",
        "# One-hot encoding for training data\n",
        "train_x_onehot = []\n",
        "for sentence in train_x_idx:\n",
        "    sentence_onehot = np.zeros(len(word_to_idx))\n",
        "    for idx in sentence:\n",
        "        sentence_onehot[idx] = 1\n",
        "    train_x_onehot.append(sentence_onehot)\n",
        "\n",
        "train_y_onehot = []\n",
        "for aspect, polarity in train_y_idx:\n",
        "    aspect_onehot = np.zeros(num_aspects)\n",
        "    aspect_onehot[aspect] = 1\n",
        "    polarity_onehot = np.zeros(num_polarities)\n",
        "    polarity_onehot[polarity] = 1\n",
        "    train_y_onehot.append((aspect_onehot, polarity_onehot))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLdoXe-tMOsb",
        "outputId": "5d190404-10e3-4772-e7eb-6faad1bc67dd"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[33], line 30\u001b[0m\n\u001b[0;32m     26\u001b[0m lemma_tokens \u001b[38;5;241m=\u001b[39m [lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m stem_tokens]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# print(\"6\", lemma_tokens)\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# POS Tagging\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m pos_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlemma_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# print(\"7\", pos_tokens)\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Reconstruct sentence\u001b[39;00m\n\u001b[0;32m     34\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemma_tokens)\n",
            "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 165\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
            "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\nltk\\tag\\__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    105\u001b[0m     tagger\u001b[38;5;241m.\u001b[39mload(ap_russian_model_loc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 107\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
            "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\nltk\\tag\\perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[0;32m    166\u001b[0m     AP_MODEL_LOC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m--> 167\u001b[0m         \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mPICKLE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload(AP_MODEL_LOC)\n",
            "File \u001b[1;32mc:\\Users\\allis\\miniconda3\\Lib\\site-packages\\nltk\\data.py:522\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[38;5;66;03m# Check each item in our path\u001b[39;00m\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path_ \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m    521\u001b[0m     \u001b[38;5;66;03m# Is the path item a zipfile?\u001b[39;00m\n\u001b[1;32m--> 522\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path_ \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m path_\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    523\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    524\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ZipFilePathPointer(path_, resource_name)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for i in range(len(train_data)):\n",
        "\n",
        "    sentence = train_data.loc[i, 'sentence']\n",
        "\n",
        "    # Lowercase\n",
        "    sentence = sentence.lower()\n",
        "    # print(\"1\", sentence)\n",
        "\n",
        "    # Tokenise\n",
        "    tokens = word_tokenize(sentence)\n",
        "    # print(\"2\", tokens)\n",
        "\n",
        "    # Remove punctuation\n",
        "    re_tokens = [remove_punctuation_re(word) for word in tokens]\n",
        "    # print(\"3\", re_tokens)\n",
        "\n",
        "    # Remove stopwords\n",
        "    sw_tokens = [word for word in re_tokens if word.lower() not in stopwords and word != '']\n",
        "    # print(\"4\", sw_tokens)\n",
        "\n",
        "    # Stemming\n",
        "    stem_tokens = [stemmer.stem(word) for word in sw_tokens]\n",
        "    # print(\"5\", stem_tokens)\n",
        "\n",
        "    # Lemmatisation\n",
        "    lemma_tokens = [lemmatizer.lemmatize(word) for word in stem_tokens]\n",
        "    # print(\"6\", lemma_tokens)\n",
        "\n",
        "    # POS Tagging\n",
        "    pos_tokens = pos_tag(lemma_tokens)\n",
        "    # print(\"7\", pos_tokens)\n",
        "\n",
        "    # Reconstruct sentence\n",
        "    sentence = \" \".join(lemma_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTHFORwcMOsc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqAsGSkNMOsc"
      },
      "source": [
        "## Pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhpH7elNMOsd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMAG9jAQMOsd"
      },
      "source": [
        "# 2. Model Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j51j5z2PMOsd"
      },
      "source": [
        "## Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0dBDZVQMOsd"
      },
      "outputs": [],
      "source": [
        "# Assuming you have already preprocessed the data and have access to the following:\n",
        "# - word embeddings (word2vec / GloVe)\n",
        "# - aspect embeddings\n",
        "# - tokenized input sentences\n",
        "# - aspect terms for each sentence\n",
        "\n",
        "aspect_terms = train_data['aspect'].tolist()\n",
        "word_embeddings = {}\n",
        "sentence_embeddings = torch.tensor([[word_embeddings.get(token, np.zeros(300)) for token in sentence.split()] for sentence in df['sentence']])\n",
        "aspect_embeddings = torch.tensor([[word_embeddings.get(token, np.zeros(300)) for token in aspect.split()] for aspect in aspect_terms])\n",
        "polarity_labels = torch.tensor([ for polarity in train_data['polarity']])\n",
        "\n",
        "# Example data (replace with your actual data)\n",
        "input_data = np.random.randint(0, 100, (10, 20))  # Example input data (batch_size=10, seq_len=20)\n",
        "aspect_terms = np.random.randint(0, 10, (10,))  # Example aspect terms (batch_size=10)\n",
        "\n",
        "# Define the model\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, num_layers, dropout):\n",
        "        super(Model1, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.randn(100, embedding_dim))  # Example vocab size=100\n",
        "        self.aspect_embedding = nn.Embedding.from_pretrained(torch.randn(10, embedding_dim))  # Example aspect vocab size=10\n",
        "        self.rnn = nn.LSTM(embedding_dim * 2, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, 3)  # Positive, Negative, Neutral\n",
        "\n",
        "    def forward(self, x, aspect):\n",
        "        embedded = self.embedding(x)\n",
        "        aspect_embedded = self.aspect_embedding(aspect).unsqueeze(1).repeat(1, x.size(1), 1)\n",
        "        combined_embedded = torch.cat((embedded, aspect_embedded), dim=2)\n",
        "        rnn_out, _ = self.rnn(combined_embedded)\n",
        "        logits = self.fc(rnn_out[:, -1, :])  # Taking the last hidden state\n",
        "        return logits\n",
        "\n",
        "# Setting Hyperparameters\n",
        "embedding_dim = 300  # Assuming 300-dimensional word embeddings\n",
        "hidden_dim = 128\n",
        "num_layers = 1\n",
        "dropout = 0.2\n",
        "learning_rate = 0.001\n",
        "num_epochs = 50\n",
        "\n",
        "# Initialize the model\n",
        "model = Model1(embedding_dim, hidden_dim, num_layers, dropout)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Convert data to tensors\n",
        "sentence_embeddings = torch.FloatTensor(sentence_embeddings)\n",
        "aspect_embeddings = torch.FloatTensor(aspect_embeddings)\n",
        "polarity = torch.\n",
        "\n",
        "= torch.LongTensor([sentiment.index(polarity) for polarity in df['polarity']])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(input_data, aspect_terms)\n",
        "    loss = criterion(outputs, torch.randint(0, 2, (10,)))  # Example target labels (binary)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtiV9R_-MOse"
      },
      "source": [
        "## Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Yh2t9BWMOse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDG1d3alMOse"
      },
      "source": [
        "## Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnqZzxz_MOse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LN2DiGh4MOse"
      },
      "source": [
        "# 3. Testing and Evaluation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cits5508-2024",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}